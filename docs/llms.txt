# Wandler - Browser-based LLM Inference Library

## Overview
Wandler is a TypeScript library that enables running LLMs directly in the browser with zero configuration. It provides a simple API for loading models and generating text, with built-in streaming support.

## Core Features
- Zero-config browser inference
- Web Worker support (default)
- Streaming responses
- TypeScript-first
- Multiple model providers support

## Key Concepts
- Models run entirely in the browser
- Web Workers handle computation by default
- Automatic caching of model weights
- Streaming uses standard Web APIs

## Provider System

Wandler automatically selects the appropriate provider based on the model path:

1. **DeepseekProvider**: Handles models from:
   - `onnx-community/deepseek/*`
   - `deepseek/*`
   
2. **TransformersProvider**: Handles models from:
   - `stabilityai/*`
   - All other models (default fallback)

The provider selection is handled automatically by `loadModel`, so you don't need to specify it manually. Each provider optimizes the model loading and inference for its specific model types.

## Basic Usage

### 1. Installation
```bash
npm install wandler
```

### 2. Loading a Model
```typescript
import { loadModel } from 'wandler';

// Load any model
const model = await loadModel("onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX", {
  onProgress: info => console.log(`${info.file}: ${info.loaded}/${info.total} bytes`)
});
```

### 3. Generating Text
```typescript
import { generateText, streamText } from 'wandler';

// Single text generation
const response = await generateText({
  model,
  messages: [
    { role: "user", content: "What is the capital of France?" }
  ]
});

// Streaming with async iterator
const result = await streamText({
  model,
  messages: [
    { role: "user", content: "Tell me a story" }
  ],
  options: {
    max_new_tokens: 1024,
    temperature: 0.7,
    top_p: 0.95,
    repetition_penalty: 1.1
  }
});

for await (const token of result) {
  console.log(token);
}
```

## Type Definitions

```typescript
// Model Options & Configuration
interface ModelOptions {
  useKV?: boolean;
  dtype?: ModelDtype;
  device?: ModelDevice;
  onProgress?: (info: ProgressInfo) => void;
}

interface ModelConfig {
  dtype: ModelDtype;
  device: ModelDevice;
  generationConfig: GenerationConfig;
}

// Model Capabilities & Performance
interface ModelCapabilities {
  textGeneration: boolean;
  textClassification: boolean;
  imageGeneration: boolean;
  audioProcessing: boolean;
  vision: boolean;
}

interface ModelPerformance {
  supportsKVCache: boolean;
  groupedQueryAttention: boolean;
  recommendedDtype: ModelDtype;
  kvCacheDtype?: Record<string, string>;
}

// Base Model & Provider System
interface BaseModel {
  id: string;
  provider: string;
  capabilities: ModelCapabilities;
  performance: ModelPerformance;
  config: Record<string, any>;
  tokenizer?: any;
  processor?: any;
  instance?: any;
  generationConfig?: GenerationConfig;
}

abstract class BaseProvider {
  abstract loadModel(modelPath: string, options?: ModelOptions): Promise<BaseModel>;
  // ... internal methods for handling progress and capabilities
}

// Message & Generation
interface Message {
  role: "user" | "assistant" | "system";
  content: string;
}

interface StreamResult<T> {
  textStream: ReadableStream<T>;
  [Symbol.asyncIterator](): AsyncIterator<T>;
  response: Promise<string>;
}

interface ProgressInfo {
  status: "progress" | "ready" | "initiate" | "download" | "done";
  file?: string;
  loaded?: number;
  total?: number;
}

interface GenerationConfig {
  max_new_tokens?: number;
  do_sample?: boolean;
  temperature?: number;
  top_p?: number;
  repetition_penalty?: number;
}

// Types
type ModelDtype = "q4f16" | "auto" | "fp32" | "fp16" | "q8" | "int8" | "uint8" | "q4" | "bnb4";
type ModelDevice = "auto" | "webgpu" | "gpu" | "cpu" | "wasm" | "cuda" | "dml" | "webnn" | "webnn-npu" | "webnn-gpu" | "webnn-cpu";
```